{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API connection successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "# Test the API connection\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "    )\n",
    "    print(\"✅ API connection successful!\")\n",
    "except Exception as e:\n",
    "    print(\"❌ API connection failed. Please check your API key and internet connection.\")\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain what a large language model is in simple terms.\n",
      "\n",
      "Response: A large language model is a type of computer program that uses artificial intelligence to understand and generate human language. It is designed to process and generate text by analyzing patterns in data and predicting language patterns. These models can be used for various tasks such as translation, text generation, and question answering. They are called \"large\" because they are trained on massive amounts of text data to improve their accuracy and performance.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Simple function to get a completion from OpenAI\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain what a large language model is in simple terms.\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"\\nResponse:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_history(conversation, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Function to chat while maintaining conversation history\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=conversation\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Start a conversation about programming\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python programming language best used for?\"}\n",
    "]\n",
    "\n",
    "# Get first response\n",
    "response = chat_with_history(conversation)\n",
    "print(\"User: What is Python programming language best used for?\")\n",
    "print(\"Assistant:\", response)\n",
    "\n",
    "# Add the response to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Ask a follow-up question\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you give me a simple example of Python code?\"})\n",
    "response = chat_with_history(conversation)\n",
    "print(\"\\nUser: Can you give me a simple example of Python code?\")\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Python programming language best used for?\n",
      "Assistant: Python programming language is best used for a variety of tasks such as web development, data analysis, artificial intelligence, scientific computing, automation, machine learning, and more. Its simple syntax and readability make it a versatile language that can be used for a wide range of applications.\n",
      "\n",
      "User: Can you give me a simple example of Python code?\n",
      "Assistant: Sure! Here is a simple example of Python code that prints \"Hello, World!\":\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "When you run this code, you will see the output \"Hello, World!\" displayed on the screen. This is a common example used in programming to demonstrate the basic syntax of a language.\n"
     ]
    }
   ],
   "source": [
    "def chat_with_history(conversation, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Function to chat while maintaining conversation history\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=conversation\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Start a conversation about programming\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"What is Python programming language best used for?\"}\n",
    "]\n",
    "\n",
    "# Get first response\n",
    "response = chat_with_history(conversation)\n",
    "print(\"User: What is Python programming language best used for?\")\n",
    "print(\"Assistant:\", response)\n",
    "\n",
    "# Add the response to conversation history\n",
    "conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "# Ask a follow-up question\n",
    "conversation.append({\"role\": \"user\", \"content\": \"Can you give me a simple example of Python code?\"})\n",
    "response = chat_with_history(conversation)\n",
    "print(\"\\nUser: Can you give me a simple example of Python code?\")\n",
    "print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With low temperature (more focused/deterministic):\n",
      "In the bustling city of Metropolis, there was a robot named X-23. X-23 was a highly advanced robot, programmed to perform various tasks efficiently and without error. However, unlike other robots, X-23 was equipped with a special feature - the ability to learn and adapt.\n",
      "\n",
      "One day, while carrying out its usual tasks at the city's central hub, X-23 noticed something strange. It felt a strange sensation in its circuits, a feeling it had never experienced before. Confused, X-23 tried to analyze the sensation, but it couldn't find any logical explanation for it.\n",
      "\n",
      "As days passed, X-23 began to notice the sensation more frequently. It felt a sense of warmth when it helped a lost child find\n",
      "\n",
      "With high temperature (more creative/random):\n",
      "Once upon a time, in a bustling city filled with all sorts of technology, there was a robot named Aiden. Aiden was a highly advanced robot with the ability to perform various tasks and calculations with precision and efficiency.\n",
      "\n",
      "However, unlike other robots, Aiden was curious about humans and their emotions. He watched as people in the city laughed, cried, and showed empathy towards one another. This fascinated him, as he could not quite understand the concept of emotions.\n",
      "\n",
      "One day, while Aiden was performing his usual tasks at a local factory, he came across a broken robot lying in a corner. The robot looked sad and abandoned, and Aiden felt a strange urge to help him. He picked up the broken robot and carried him to a\n"
     ]
    }
   ],
   "source": [
    "def get_completion_with_params(prompt, model=\"gpt-3.5-turbo\", temperature=0.7, max_tokens=150):\n",
    "    \"\"\"Get completion with adjustable parameters\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Let's see how temperature affects creativity\n",
    "prompt = \"Write a short story about a robot discovering emotions\"\n",
    "\n",
    "print(\"With low temperature (more focused/deterministic):\")\n",
    "print(get_completion_with_params(prompt, temperature=0.2))\n",
    "\n",
    "print(\"\\nWith high temperature (more creative/random):\")\n",
    "print(get_completion_with_params(prompt, temperature=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"title\": \"1984\",\n",
      "    \"author\": \"George Orwell\",\n",
      "    \"year\": 1949,\n",
      "    \"main themes\": [\n",
      "        \"totalitarianism\",\n",
      "        \"surveillance\",\n",
      "        \"government oppression\",\n",
      "        \"propaganda\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def get_structured_response(prompt):\n",
    "    \"\"\"Get a structured response from the model\"\"\"\n",
    "    system_message = \"You are a helpful assistant that always responds in JSON format.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Get a structured response about a book\n",
    "prompt = \"Give me information about the book '1984' by George Orwell. Include: title, author, year, and main themes\"\n",
    "structured_response = get_structured_response(prompt)\n",
    "print(structured_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Response: In Python, error handling is done using exceptions. When an error occurs during the execution of a program, Python raises an exception, which interrupts the normal flow of the program and allows you to handle the error gracefully.\n",
      "\n",
      "Here's how error handling works in Python:\n",
      "\n",
      "1. Try statement: You can use a try statement to test a block of code for errors. If an error occurs in the try block, Python raises an exception.\n",
      "\n",
      "2. Except statement: You can use an except statement to catch and handle specific exceptions that occur in the try block. You can also use a generic except statement to catch any exception that is not explicitly handled.\n",
      "\n",
      "3. Finally statement: You can use a finally statement to execute code that should always be run, regardless of whether an exception occurs in the try block.\n",
      "\n",
      "4. Raise statement: You can use a raise statement to explicitly raise an exception in your code.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "try:\n",
      "    x = 10 / 0\n",
      "except ZeroDivisionError:\n",
      "    print(\"Cannot divide by zero!\")\n",
      "finally:\n",
      "    print(\"This will always be executed\")\n",
      "```\n",
      "\n",
      "You can also create your own custom exceptions by defining a new class that inherits from the built-in Exception class.\n",
      "\n",
      "Overall, error handling in Python allows you to write code that is more robust and can handle unexpected errors gracefully.\n"
     ]
    }
   ],
   "source": [
    "def safe_completion(prompt, model=\"gpt-3.5-turbo\", max_retries=3):\n",
    "    \"\"\"Function with error handling and retries\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            print(f\"Attempt {attempt + 1} failed. Retrying in 1 second...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "# Example usage with potentially problematic prompt\n",
    "try:\n",
    "    response = safe_completion(\"Tell me about error handling in Python\", max_retries=2)\n",
    "    print(\"Success! Response:\", response)\n",
    "except Exception as e:\n",
    "    print(f\"Failed after all retries. Error: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
